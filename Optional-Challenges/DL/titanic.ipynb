{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Challenge (optional)\n",
    "\n",
    "![](../../public/titanic_intro.png)\n",
    "\n",
    "In the early 20th century, the RMS Titanic was the pinnacle of luxury and innovation, a marvel of modern engineering. It was hailed as the \"unsinkable\" ship, carrying over 2,200 passengers and crew on its maiden voyage across the Atlantic. However, in the icy waters of the North Atlantic, disaster struck, and the unthinkable happened—the Titanic collided with an iceberg and sank, leading to one of the most tragic maritime disasters in history.\n",
    "\n",
    "Now, over a century later, you are tasked with an important mission: to delve into the historical data and build a predictive model that could have foretold the fate of the passengers aboard the Titanic. This dataset contains detailed records of the passengers, including information such as age, gender, ticket class, family size, and more. **Your goal is to develop a neural network model that accurately predicts whether a passenger would have survived or perished on that fateful night.**\n",
    "\n",
    "Your predictive model won't just be a technical achievement; it will serve as a lens through which we can better understand the human factors and decisions that played a critical role in survival. As you work through this challenge, you’ll follow the standard deep learning workflow, applying your skills to each stage:\n",
    "\n",
    "- Data Collection: The data you need has already been gathered from historical records.\n",
    "- Data Preprocessing: Clean and prepare the data for analysis (partially done for you).\n",
    "- Exploratory Data Analysis (EDA): Investigate the data and uncover key patterns (partially done for you).\n",
    "- Feature Engineering: Create or modify features to enhance your model’s performance (paritally done for you).\n",
    "- Model Architecture Design: Choose an appropriate structure for your neural network model.\n",
    "- Training: Train your model using the provided dataset.\n",
    "- Evaluation: Assess your model's accuracy using a validation set and other techniques.\n",
    "- Hyperparameter Tuning: Fine-tune the model’s parameters to improve performance.\n",
    "- Model Testing: Test your final model on a separate test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sibsp        # number of siblings / spouses aboard the Titanic \t\n",
    "# parch        # number of parents / children aboard the Titanic \t\n",
    "# ticket       # Ticket number \t\n",
    "# fare         # Passenger fare\n",
    "# cabin        # Cabin number\n",
    "# embark_town  # Port of Embarkation \t\n",
    "\n",
    "#sibsp: The dataset defines family relations in this way:\n",
    "#Sibling = brother, sister, stepbrother, stepsister\n",
    "#Spouse = husband, wife (mistresses and fiancés were ignored)\n",
    "\n",
    "#parch: The dataset defines family relations in this way:\n",
    "#Parent = mother, father\n",
    "#Child = daughter, son, stepdaughter, stepson\n",
    "#Some children travelled only with a nanny, therefore parch=0 for them.\n",
    "\n",
    "# load titanic dataset (DO NOT MODIFY)\n",
    "df = sns.load_dataset(\"titanic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "Provided below is some starter code to help familiarize yourself with the Titanic dataset. Further data analysis is encouraged but not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "\n",
    "##############################\n",
    "# Uncomment code to see output\n",
    "##############################\n",
    "\n",
    "# df.shape\n",
    "# df.isna().sum()\n",
    "# df.describe()\n",
    "\n",
    "## Pie Chart for Survived\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.pie(df[\"survived\"].value_counts(), labels=[f\"Did Not Survive ({df.shape[0]-df['survived'].sum()})\", f\"Survived ({df['survived'].sum()})\"], autopct='%1.1f%%')\n",
    "# plt.title('Survival Count')\n",
    "# plt.show()\n",
    "\n",
    "## Pie Chart for Pclass\n",
    "# pclass_counts = df['pclass'].value_counts()\n",
    "# labels = [f'Class {cls}: {count} passengers' for cls, count in zip(pclass_counts.index, pclass_counts.values)]\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.pie(pclass_counts, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "# plt.title('Distribution of Passengers by Class')\n",
    "# plt.show()\n",
    "\n",
    "## Distribution of Age\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.histplot(df['age'].dropna(), kde=True, bins=30, color='blue')\n",
    "# plt.title('Age Distribution of Passengers')\n",
    "# plt.xlabel('Age')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "## Survival by Sex\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.countplot(x='sex', hue='survived', data=df, palette='Set1')\n",
    "# plt.title('Survival by Sex')\n",
    "# plt.xlabel('Sex')\n",
    "# plt.ylabel('Count')\n",
    "# plt.show()\n",
    "\n",
    "## Survival by Passenger Class\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.countplot(x='pclass', hue='survived', data=df, palette='Set2')\n",
    "# plt.title('Survival by Passenger Class')\n",
    "# plt.xlabel('Passenger Class')\n",
    "# plt.ylabel('Count')\n",
    "# plt.show()\n",
    "\n",
    "## Survival by Embark Town\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.countplot(x='embark_town', hue='survived', data=df, palette='Set1')\n",
    "# plt.title('Survival by Embarkation Town')\n",
    "# plt.xlabel('Embarkation Town')\n",
    "# plt.ylabel('Count')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that are redundant or contain many NaN values\n",
    "df = df.drop([\"pclass\", \"alive\", \"embarked\", \"alone\", \"adult_male\", \"deck\", \"age\"], axis = 1)\n",
    "df = df.dropna(subset=[\"embark_town\"])\n",
    "\n",
    "# TODO: Further data preprocessing (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encode categorical variables\n",
    "df[\"sex\"] = df[\"sex\"].map({\"male\": 0, \"female\": 1})\n",
    "for label in [\"class\", \"who\", \"embark_town\"]:\n",
    "    df = df.join(pd.get_dummies(df[label], prefix=label))\n",
    "    df = df.drop(label, axis=1)\n",
    "\n",
    "# TODO: Further feature engineering (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Development\n",
    "\n",
    "Check out the sections below marked `TODO` to get started on developing your model. You are free to modify and reformat any of the code provided as you see fit.\n",
    "\n",
    "\n",
    "We will be using PyTorch for creating our deep learning model. \n",
    "### PyTorch Resources:\n",
    "[torch.nn documentation](https://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "[torch.nn.Sequential() documentation](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# feel free to add any additional packages as you see fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --------------------------------------------\n",
    "### TODO 1/4: modify train/validation/test split\n",
    "### --------------------------------------------\n",
    "\n",
    "train_size = 0.5\n",
    "validation_size = 0.3\n",
    "test_size = 0.2\n",
    "\n",
    "# Note the inclusion of a validation set. This allows \n",
    "# for unbiased evaluation during model development,\n",
    "# ensuring the test data remains unseen until final\n",
    "# assessment. The validation set is used repeatedly\n",
    "# during the model tuning process, whereas the test\n",
    "# set provides an independent evaluation once the\n",
    "# model is finalized.\n",
    "\n",
    "### --------------------------------------------END TODO 1/4\n",
    "\n",
    "X = df.drop(columns=['survived'])\n",
    "y = df['survived']\n",
    "\n",
    "def to_torch_tensor(X: pd.DataFrame, y: pd.Series) -> torch.Tensor:\n",
    "    return (torch.tensor(X.to_numpy(dtype=np.float32), dtype=torch.float32),\n",
    "            torch.tensor(y.to_numpy(dtype=np.float32), dtype=torch.float32).unsqueeze(1))\n",
    "\n",
    "X, y = to_torch_tensor(X, y)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size, random_state=0, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, train_size=validation_size/(test_size+validation_size), random_state=0, stratify=y_test)\n",
    "\n",
    "print(f'Training set size: {len(X_train)}')\n",
    "print(f'Validation set size: {len(X_val)}')\n",
    "print(f'Test set size: {len(X_test)}')\n",
    "\n",
    "# (optional) Standardize the data (using the training mean and std)\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "scaler = StandardScaler()\n",
    "X_train = torch.tensor(scaler.fit_transform(X_train), dtype=torch.float32)\n",
    "X_val = torch.tensor(scaler.transform(X_val), dtype=torch.float32)\n",
    "X_test = torch.tensor(scaler.transform(X_test), dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "class TitanicNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TitanicNN, self).__init__()\n",
    "        ### ------------------------------------------------\n",
    "        ### TODO 2/4: Define model architecture here\n",
    "        ### ------------------------------------------------\n",
    "        # Linear / Fully-Connected Layer:\n",
    "\n",
    "        # Regularization Techniques:\n",
    "\n",
    "        # Activation Functions:\n",
    "        \n",
    "        #...\n",
    "        ### ------------------------------------------------END TODO 2/4\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ### ------------------------------------------------\n",
    "        ### TODO 3/4: Define forward pass here\n",
    "        ### ------------------------------------------------\n",
    "\n",
    "        return x\n",
    "        ### ------------------------------------------------END TODO 3/4\n",
    "\n",
    "# Alternative method of defining your model:\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(in_features=X.shape[1], out_features=1),\n",
    "#     ...,\n",
    "#     )\n",
    "\n",
    "model = TitanicNN()\n",
    "\n",
    "### ---------------------------------------------------------------------------\n",
    "### TODO 4/4: modify hyperparameters (choice of optimizer, learning rate, etc.)\n",
    "### ---------------------------------------------------------------------------\n",
    "\n",
    "# Epochs: the number of complete passes of the training dataset\n",
    "epochs = 5\n",
    "\n",
    "# Batch Size: the number of training samples\n",
    "# in one forward/backward pass. Smaller batch size\n",
    "# may result in more noise when calculating errors,\n",
    "# while larger batch sizes obtain a more accurate\n",
    "# estimate of the true gradient at the cost of\n",
    "# computation time\n",
    "batch_size = 32\n",
    "\n",
    "# Learning Rate: determines step size at each\n",
    "# iteration of optimizer\n",
    "lr = 1e-5\n",
    "\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0, amsgrad=False)\n",
    "\n",
    "### ---------------------------------------------------------------------------END TODO 4/4\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=y_test.shape[0], shuffle=True)\n",
    "\n",
    "def train_validate(train_loader: DataLoader, val_loader: DataLoader, model: nn.Module, loss_function: nn.Module, optimizer: torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Runs a single epoch of training and validation for a given model.\n",
    "\n",
    "    This function trains the model for one epoch using the provided training data, \n",
    "    computes the loss, performs backpropagation, and updates the model's parameters. \n",
    "    It also evaluates the model on the validation set and returns the average training \n",
    "    and validation losses and accuracies for the epoch.\n",
    "\n",
    "    Args:\n",
    "        train_loader: DataLoader for the training dataset. \n",
    "            Contains batches of input data and corresponding labels.\n",
    "        val_loader: DataLoader for the validation dataset. \n",
    "            Contains batches of input data and corresponding labels.\n",
    "        model: The neural network model to be trained and validated.\n",
    "        loss_function: Loss function to be used for computing the loss. \n",
    "            For example, `torch.nn.CrossEntropyLoss` or `torch.nn.BCELoss`.\n",
    "        optimizer: Optimizer for updating the model's parameters \n",
    "            based on the computed gradients. For example, `torch.optim.Adam` or `torch.optim.SGD`.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - train_loss (float): The average loss over the training dataset for this epoch.\n",
    "            - val_loss (float): The average loss over the validation dataset for this epoch.\n",
    "            - train_accuracy (float): The accuracy over the training dataset for this epoch.\n",
    "            - val_accuracy (float): The accuracy over the validation dataset for this epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    size = len(train_loader.dataset)\n",
    "    model.train()\n",
    "    running_loss, train_accuracy = 0, 0\n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_function(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # Convert probabilities to binary predictions\n",
    "        pred_binary = (pred > 0.5).float()\n",
    "        correct_predictions = (pred_binary.squeeze(1) == y.squeeze(1)).float().sum().item()\n",
    "        train_accuracy += correct_predictions\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy /= size\n",
    "\n",
    "    size = len(val_loader.dataset)\n",
    "    num_batches = len(val_loader)\n",
    "    model.eval()\n",
    "    val_loss, val_accuracy = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            pred = model(X)\n",
    "            val_loss += loss_function(pred, y).item()\n",
    "\n",
    "            # Convert probabilities to binary predictions\n",
    "            pred_binary = (pred > 0.5).float()\n",
    "            correct_predictions = (pred_binary.squeeze(1) == y.squeeze(1)).float().sum().item()\n",
    "            val_accuracy += correct_predictions\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    val_accuracy /= size\n",
    "    return train_loss, val_loss, train_accuracy, val_accuracy\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "for t in range(epochs):\n",
    "    train_loss, val_loss, train_accuracy, val_accuracy = train_validate(train_loader, val_loader, model, loss_function, optimizer)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "def plot_train_curves(train_losses: list[float], val_losses: list[float], train_accuracies: list[float], val_accuracies: list[float]) -> None:\n",
    "    plt.figure(figsize=(16, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "    plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Train Loss: {train_losses[-1]:.4f}')\n",
    "    print(f'Validation Loss: {val_losses[-1]:.4f}')\n",
    "    print(f'Train Accuracy: {train_accuracies[-1]:.4f}')\n",
    "    print(f'Validation Accuracy: {val_accuracies[-1]:.4f}')\n",
    "\n",
    "plot_train_curves(train_losses, val_losses, train_accuracies, val_accuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing\n",
    "\n",
    "After training and optimizing your model, run the below cell to test it on your test dataset. The test function have already been written for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, loss_function):\n",
    "    size = len(test_loader.dataset)\n",
    "    num_batches = len(test_loader)\n",
    "    model.eval()\n",
    "    test_loss, correct_predictions = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_function(pred, y).item()\n",
    "\n",
    "            # Convert probabilities to binary predictions\n",
    "            pred_binary = (pred > 0.5).float()\n",
    "            correct_predictions += (pred_binary.squeeze(1) == y.squeeze(1)).float().sum().item()\n",
    "    \n",
    "    # Average loss and accuracy\n",
    "    test_loss /= num_batches\n",
    "    test_accuracy = correct_predictions / size * 100  # Convert to percentage\n",
    "\n",
    "    print(f\"Test Set Metrics:\\n Accuracy: {test_accuracy:>0.1f}%\\n Loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=y_test.shape[0], shuffle=True)\n",
    "test(test_loader, model, loss_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
